{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un **RDD** es una estructura de datos fundamental en Apache Spark. Piensa en él como una **cinta transportadora** que distribuye los datos entre varias máquinas para que puedan ser procesados de manera paralela (¡muy rápido!). Es muy útil para realizar operaciones con grandes volúmenes de datos de forma eficiente.\n",
    "\n",
    "**Resilient** significa que los RDDs pueden **recuperarse** de errores si alguna parte de la operación falla. **Distributed** significa que los datos pueden estar distribuidos en varias máquinas, y **Dataset** se refiere a un conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuentes de datos a las que prodrémos conectaros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Colecciones locales**\n",
    "  - Listas o arrays en Python usando `parallelize`.\n",
    "\n",
    "- **Archivos locales o distribuidos**\n",
    "  - Archivos de texto (`.txt`).\n",
    "  - Archivos CSV.\n",
    "  - Archivos JSON.\n",
    "  - Archivos Parquet.\n",
    "  - Archivos ORC.\n",
    "  - Archivos en HDFS.\n",
    "  - Archivos en S3 (Amazon Simple Storage Service).\n",
    "  - Archivos en Azure Blob Storage.\n",
    "\n",
    "- **Bases de datos relacionales**\n",
    "  - MySQL.\n",
    "  - PostgreSQL.\n",
    "  - Oracle.\n",
    "  - Microsoft SQL Server.\n",
    "  - SQLite.\n",
    "\n",
    "- **Bases de datos NoSQL**\n",
    "  - Cassandra.\n",
    "  - MongoDB.\n",
    "  - HBase.\n",
    "  - Redis.\n",
    "\n",
    "- **Streams y mensajes**\n",
    "  - Kafka.\n",
    "  - Flume.\n",
    "  - Sockets TCP/UDP.\n",
    "\n",
    "- **APIs externas**\n",
    "  - REST APIs.\n",
    "  - Servicios web SOAP.\n",
    "\n",
    "- **Otras fuentes**\n",
    "  - Elasticsearch.\n",
    "  - Google BigQuery.\n",
    "  - Snowflake.\n",
    "  - Data Lakes (genéricos).\n",
    "  - Hive Metastore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traemos nuestros datos de ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/11 19:16:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"CSV to RDD\")\n",
    "clientes_ruta_csv = \"../resources/clientes.csv\"\n",
    "ordenes_ruta_csv = \"../resources/ordenes.csv\"\n",
    "\n",
    "rdd_clientes = sc.textFile(clientes_ruta_csv)\n",
    "rdd_ordenes = sc.textFile(ordenes_ruta_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertir nuestro archivo en filas y columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En caso de que estemos trabajando con datos tabulados debemos separarlos para poder operar como si se tratará de filas y columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd_clientes = rdd_clientes.map(lambda line: line.split(\",\"))\n",
    "rdd_ordenes = rdd_ordenes.map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Los encabezados de nuestro csv no son relevantes en este caso por lo que los vamos a eliminar\n",
    "\n",
    "# Extraer los encabezados una sola vez\n",
    "header_clientes = rdd_clientes.first()\n",
    "header_ordenes = rdd_ordenes.first()\n",
    "\n",
    "# Filtrar las filas para eliminar los encabezados\n",
    "rdd_clientes = rdd_clientes.filter(lambda x: x != header_clientes)\n",
    "rdd_ordenes = rdd_ordenes.filter(lambda x: x != header_ordenes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostrar un determinado numero de filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0', '1', '9743', '365.04', '2022-02-16', 'Cancelada'],\n",
       " ['1', '2', '6515', '750.04', '2022-01-28', 'Cancelada'],\n",
       " ['2', '3', '9812', '167.25', '2023-10-31', 'Cancelada']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_ordenes.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0',\n",
       "  '1',\n",
       "  'Tyler Jones',\n",
       "  'johnsonjuan@example.net',\n",
       "  '2020-05-15',\n",
       "  'Henrytown']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_clientes.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicar una transforación a los elementos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para modificar los valorres dentro de nuestro RDD debemos de aplicarle una función  que nos ayude a transformalo, esto lo podemos lograr con .map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[69.3576, 142.5076, 31.7775]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# por ejemplo si queremos calcula el valor del iva en las ordenes:\n",
    "\n",
    "iva = rdd_ordenes.map(lambda x: float(x[3])*0.19)\n",
    "iva.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0', '1', '9743', '365.04', '2022-02-16', 'Cancelada', 69.3576],\n",
       " ['1', '2', '6515', '750.04', '2022-01-28', 'Cancelada', 142.5076],\n",
       " ['2', '3', '9812', '167.25', '2023-10-31', 'Cancelada', 31.7775],\n",
       " ['3', '4', '324', '351.19', '2024-02-03', 'Pendiente', 66.7261],\n",
       " ['4', '5', '8519', '466.03', '2024-02-10', 'Pendiente', 88.5457]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Si quieremos agregar una culomna con el iva\n",
    "rdd_ordenes = rdd_ordenes.map(lambda x: x + [float(x[3])*0.19])\n",
    "rdd_ordenes.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrar filas por dada una condición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pdemos filtrar por nuestras columnas por ejemplo\n",
    "\n",
    "# Filtro por fecha\n",
    "rdd_ordenes_dates = rdd_ordenes.filter(lambda x: x[4] == '2024-02-01')\n",
    "\n",
    "# Filtrar por estado\n",
    "rdd_ordenes_status = rdd_ordenes.filter(lambda x: x[5] == 'Pendiente')\n",
    "rdd_ordenes_status.take(5)\n",
    "\n",
    "# Filtrar por dos condiciones\n",
    "rdd_ordenes_double_check = rdd_ordenes.filter(lambda x: (x[5] == 'Pendiente') and (x[4] == '2024-02-01'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregar valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:===================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263005168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Queremos saber el total de compras aprovadas para el año 2024\n",
    "response = rdd_ordenes.filter(\n",
    "    lambda x: (x[5]=='Completada') and \n",
    "    (x[4]>='2024-01-01') and\n",
    "    (x[4]<='2024-12-31')).map(\n",
    "        lambda x: float(x[3])).reduce(lambda x, y: x+y)\n",
    "print(int(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregar valores por una clave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:===================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('8', 'Pendiente'), 765.72), (('14', 'Completada'), 942.59), (('17', 'Cancelada'), 791.26), (('19', 'Pendiente'), 788.49), (('22', 'Cancelada'), 376.08), (('26', 'Cancelada'), 11.24), (('27', 'Pendiente'), 709.15), (('44', 'Cancelada'), 361.51), (('55', 'Completada'), 853.29), (('76', 'Completada'), 849.3)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Podemos agregar  nuestros valores por uno o mas llaves, por ejemplo, vamos a agregar por usuario y por el status de la transacción\n",
    "\n",
    "response = rdd_ordenes.map(lambda x: ((x[1], x[5]), float(x[3])))  # Tupla de 2 elementos como clave\n",
    "response = response.reduceByKey(lambda x, y: x + y)  # Reducir por clave\n",
    "#print(response.collect())\n",
    "print(response.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flat map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este nos permite aplanar datos que contengan varias dimesiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '0', '1', '1', '2', '2', '3', '3', '4', '4']\n"
     ]
    }
   ],
   "source": [
    "# En este ejemplo estamos creando un rdd con registros de una dimensión, flatmap los desglosa y los convierte en elementos dse 0 dim ( por eso se repiden )\n",
    "response = rdd_clientes.map(lambda x: [x[0], x[0]]).flatMap(lambda x: x)\n",
    "print(response.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agrupar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Henrytown', <pyspark.resultiterable.ResultIterable at 0x105862a90>),\n",
       " ('Ballardshire', <pyspark.resultiterable.ResultIterable at 0x105863ed0>),\n",
       " ('Port Daltonfort', <pyspark.resultiterable.ResultIterable at 0x105862650>),\n",
       " ('Mercerberg', <pyspark.resultiterable.ResultIterable at 0x105862910>),\n",
       " ('South Jacob', <pyspark.resultiterable.ResultIterable at 0x1058624d0>)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vamos a agrupar los cleientes por ciudad\n",
    "rdd_clientes.map(lambda x: (x[5], x[0])).groupBy(lambda x: x[0]).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unir dos conjuntos de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('9743', ('0', 'East Gregburgh')),\n",
       " ('9743', ('3173', 'East Gregburgh')),\n",
       " ('9743', ('15866', 'East Gregburgh')),\n",
       " ('9743', ('33948', 'East Gregburgh')),\n",
       " ('9743', ('35045', 'East Gregburgh'))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_clientes_join = rdd_clientes.map(lambda x: (x[1],x[5]))\n",
    "rdd_ordenes_join = rdd_ordenes.map(lambda x: (x[2], x[0]))\n",
    "response = rdd_ordenes_join.leftOuterJoin(rdd_clientes_join)\n",
    "response.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hallar valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Los valores nulos ó faltantes en este caso están representados por \"\"\n",
    "response = rdd_ordenes.filter(lambda x: \"\" in x)\n",
    "response.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hallar valores unicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['johnsonjuan@example.net',\n",
       " 'qramirez@example.com',\n",
       " 'vasquezshawna@example.com',\n",
       " 'vjones@example.net',\n",
       " 'silvalisa@example.org']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hallamos los valores unicos de los correos de nuestros clientes\n",
    "response = rdd_clientes.map(lambda x: x[3]).distinct()\n",
    "response.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar nuestro rdd en cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[39] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_ordenes.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manejo de errores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ya sabemos pySapark es tolerante a fallos, o que nos permite gestionar errores sin necesidad de ejecutar todo nuestro proceso desde el inicio. Para gestionarlo la librería nos proporciona herramientas como:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinar numero maximo de fallas y otras configuraciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark divide nuestros datos en nodos; si en uno de ellos ocurre un error, se volverá a ejecutar solo ese nodo el numero de veces que definamos hasta que complete el maximo. En caso de que sobrepase el numero defiinido fallará toda la operación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "# Configuración para reintentar tareas 4 veces\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.task.maxFailures\", 4) \\\n",
    "    .config(\"spark.speculation\", \"true\") \\\n",
    "    .config(\"spark.speculation.interval\", \"100ms\") \\\n",
    "    .config(\"spark.speculation.multiplier\", 1.5) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **maxFailures:** Numero de veces hasta que nos marque error\n",
    "- **speculation:** Spark puede iniciar ejecuciones paralelas de tareas que se están ejecutando lentamente.\n",
    "- **speculation.interval:** Define el intervalo de tiempo entre comprobaciones para identificar tareas lentas.\n",
    "- **speculation.multiplier:** Determina cuándo se considera que una tarea está lo suficientemente lenta como para iniciar una tarea especulativa (lo calculamos en veces respecto al promedio de tareas similares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manejo de errores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esto podemos usar una estructura simple try, exception, finally para manejar errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/11 19:17:25 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la operación: [PATH_NOT_FOUND] Path does not exist: file:/Users/newo/Mateo_BP/data_engineering/pyspark_activities/practical_exercise/datos.csv.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ToleranciaErrores\") \\\n",
    "    .config(\"spark.task.maxFailures\", 4) \\\n",
    "    .config(\"spark.speculation\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    # Operaciones de Spark\n",
    "    resultado = spark.read.csv(\"datos.csv\") \\\n",
    "        .filter(algunaCondicion) \\\n",
    "        .collect()\n",
    "except Exception as error:\n",
    "    print(f\"Error en la operación: {error}\")\n",
    "    # Manejo de error\n",
    "finally:\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones con decoradores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "import time\n",
    "\n",
    "def medir_tiempo(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        inicio = time.time()\n",
    "        resultado = func(*args, **kwargs)\n",
    "        fin = time.time()\n",
    "        print(f\"{func.__name__} tardó {fin - inicio} segundos\")\n",
    "        return resultado\n",
    "    return wrapper\n",
    "\n",
    "@medir_tiempo\n",
    "def calcular_factorial(n):\n",
    "    \"\"\"Calcula el factorial de un número\"\"\"\n",
    "    if n < 0:\n",
    "        return None\n",
    "    resultado = 1\n",
    "    for i in range(1, n + 1):\n",
    "        resultado *= i\n",
    "    return resultado\n",
    "\n",
    "# Demostración\n",
    "print(calcular_factorial.__name__)\n",
    "print(calcular_factorial.__doc__)\n",
    "print(calcular_factorial(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de la función:  sumar\n",
      "Resultado:  2\n",
      "Tiempo de ejecución:  7.152557373046875e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import wraps\n",
    "import time\n",
    "\n",
    "def metadata(func):\n",
    "    @wraps(func)\n",
    "    def funcion(*args, **kwargs):\n",
    "        # Medir tiempo de inicio\n",
    "        inicio = time.time()\n",
    "        \n",
    "        # Ejecutar la función original\n",
    "        resultado = func(*args, **kwargs)\n",
    "        \n",
    "        # Medir tiempo final\n",
    "        fin = time.time()\n",
    "\n",
    "        # Imprimir metadatos\n",
    "        print('Nombre de la función: ', func.__name__)\n",
    "        print('Resultado: ', resultado)\n",
    "        print('Tiempo de ejecución: ', fin - inicio)\n",
    "\n",
    "        return resultado\n",
    "    return funcion\n",
    "\n",
    "@metadata\n",
    "def sumar(a,b):\n",
    "    return a + b\n",
    "\n",
    "sumar(1,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
