{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un **RDD** es una estructura de datos fundamental en Apache Spark. Piensa en él como una **cinta transportadora** que distribuye los datos entre varias máquinas para que puedan ser procesados de manera paralela (¡muy rápido!). Es muy útil para realizar operaciones con grandes volúmenes de datos de forma eficiente.\n",
    "\n",
    "**Resilient** significa que los RDDs pueden **recuperarse** de errores si alguna parte de la operación falla. **Distributed** significa que los datos pueden estar distribuidos en varias máquinas, y **Dataset** se refiere a un conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuentes de datos a las que prodrémos conectaros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Colecciones locales**\n",
    "  - Listas o arrays en Python usando `parallelize`.\n",
    "\n",
    "- **Archivos locales o distribuidos**\n",
    "  - Archivos de texto (`.txt`).\n",
    "  - Archivos CSV.\n",
    "  - Archivos JSON.\n",
    "  - Archivos Parquet.\n",
    "  - Archivos ORC.\n",
    "  - Archivos en HDFS.\n",
    "  - Archivos en S3 (Amazon Simple Storage Service).\n",
    "  - Archivos en Azure Blob Storage.\n",
    "\n",
    "- **Bases de datos relacionales**\n",
    "  - MySQL.\n",
    "  - PostgreSQL.\n",
    "  - Oracle.\n",
    "  - Microsoft SQL Server.\n",
    "  - SQLite.\n",
    "\n",
    "- **Bases de datos NoSQL**\n",
    "  - Cassandra.\n",
    "  - MongoDB.\n",
    "  - HBase.\n",
    "  - Redis.\n",
    "\n",
    "- **Streams y mensajes**\n",
    "  - Kafka.\n",
    "  - Flume.\n",
    "  - Sockets TCP/UDP.\n",
    "\n",
    "- **APIs externas**\n",
    "  - REST APIs.\n",
    "  - Servicios web SOAP.\n",
    "\n",
    "- **Otras fuentes**\n",
    "  - Elasticsearch.\n",
    "  - Google BigQuery.\n",
    "  - Snowflake.\n",
    "  - Data Lakes (genéricos).\n",
    "  - Hive Metastore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traemos nuestros datos de ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/10 20:45:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"CSV to RDD\")\n",
    "clientes_ruta_csv = \"../resources/clientes.csv\"\n",
    "ordenes_ruta_csv = \"../resources/ordenes.csv\"\n",
    "\n",
    "rdd_clientes = sc.textFile(clientes_ruta_csv)\n",
    "rdd_ordenes = sc.textFile(ordenes_ruta_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertir nuestro archivo en filas y columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En caso de que estemos trabajando con datos tabulados debemos separarlos para poder operar como si se tratará de filas y columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd_clientes = rdd_clientes.map(lambda line: line.split(\",\"))\n",
    "rdd_ordenes = rdd_ordenes.map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Los encabezados de nuestro csv no son relevantes en este caso por lo que los vamos a eliminar\n",
    "\n",
    "# Extraer los encabezados una sola vez\n",
    "header_clientes = rdd_clientes.first()\n",
    "header_ordenes = rdd_ordenes.first()\n",
    "\n",
    "# Filtrar las filas para eliminar los encabezados\n",
    "rdd_clientes = rdd_clientes.filter(lambda x: x != header_clientes)\n",
    "rdd_ordenes = rdd_ordenes.filter(lambda x: x != header_ordenes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mostrar un determinado numero de filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0', '1', '9743', '365.04', '2022-02-16', 'Cancelada'],\n",
       " ['1', '2', '6515', '750.04', '2022-01-28', 'Cancelada'],\n",
       " ['2', '3', '9812', '167.25', '2023-10-31', 'Cancelada']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_ordenes.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0',\n",
       "  '1',\n",
       "  'Tyler Jones',\n",
       "  'johnsonjuan@example.net',\n",
       "  '2020-05-15',\n",
       "  'Henrytown']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_clientes.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicar una transforación a los elementos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para modificar los valorres dentro de nuestro RDD debemos de aplicarle una función  que nos ayude a transformalo, esto lo podemos lograr con .map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[69.3576, 142.5076, 31.7775]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# por ejemplo si queremos calcula el valor del iva en las ordenes:\n",
    "\n",
    "iva = rdd_ordenes.map(lambda x: float(x[3])*0.19)\n",
    "iva.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0', '1', '9743', '365.04', '2022-02-16', 'Cancelada', 69.3576],\n",
       " ['1', '2', '6515', '750.04', '2022-01-28', 'Cancelada', 142.5076],\n",
       " ['2', '3', '9812', '167.25', '2023-10-31', 'Cancelada', 31.7775],\n",
       " ['3', '4', '324', '351.19', '2024-02-03', 'Pendiente', 66.7261],\n",
       " ['4', '5', '8519', '466.03', '2024-02-10', 'Pendiente', 88.5457]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Si quieremos agregar una culomna con el iva\n",
    "rdd_ordenes = rdd_ordenes.map(lambda x: x + [float(x[3])*0.19])\n",
    "rdd_ordenes.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrar filas por dada una condición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pdemos filtrar por nuestras columnas por ejemplo\n",
    "\n",
    "# Filtro por fecha\n",
    "rdd_ordenes_dates = rdd_ordenes.filter(lambda x: x[4] == '2024-02-01')\n",
    "\n",
    "# Filtrar por estado\n",
    "rdd_ordenes_status = rdd_ordenes.filter(lambda x: x[5] == 'Pendiente')\n",
    "rdd_ordenes_status.take(5)\n",
    "\n",
    "# Filtrar por dos condiciones\n",
    "rdd_ordenes_double_check = rdd_ordenes.filter(lambda x: (x[5] == 'Pendiente') and (x[4] == '2024-02-01'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregar valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:============================================>              (6 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263005168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Queremos saber el total de compras aprovadas para el año 2024\n",
    "response = rdd_ordenes.filter(\n",
    "    lambda x: (x[5]=='Completada') and \n",
    "    (x[4]>='2024-01-01') and\n",
    "    (x[4]<='2024-12-31')).map(\n",
    "        lambda x: float(x[3])).reduce(lambda x, y: x+y)\n",
    "print(int(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregar valores por una clave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:===================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('8', 'Pendiente'), 765.72), (('14', 'Completada'), 942.59), (('17', 'Cancelada'), 791.26), (('19', 'Pendiente'), 788.49), (('22', 'Cancelada'), 376.08), (('26', 'Cancelada'), 11.24), (('27', 'Pendiente'), 709.15), (('44', 'Cancelada'), 361.51), (('55', 'Completada'), 853.29), (('76', 'Completada'), 849.3)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Podemos agregar  nuestros valores por uno o mas llaves, por ejemplo, vamos a agregar por usuario y por el status de la transacción\n",
    "\n",
    "response = rdd_ordenes.map(lambda x: ((x[1], x[5]), float(x[3])))  # Tupla de 2 elementos como clave\n",
    "response = response.reduceByKey(lambda x, y: x + y)  # Reducir por clave\n",
    "#print(response.collect())\n",
    "print(response.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flat map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este nos permite aplanar datos que contengan varias dimesiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '0', '1', '1', '2', '2', '3', '3', '4', '4']\n"
     ]
    }
   ],
   "source": [
    "# En este ejemplo estamos creando un rdd con registros de una dimensión, flatmap los desglosa y los convierte en elementos dse 0 dim ( por eso se repiden )\n",
    "response = rdd_clientes.map(lambda x: [x[0], x[0]]).flatMap(lambda x: x)\n",
    "print(response.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
