{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es un módulo de Apache Spark diseñado para procesar flujos de datos en tiempo real de manera escalable, tolerante a fallos y con un enfoque de alta abstracción. Trata los datos en tiempo real como si fueran una tabla continua en constante crecimiento. Puedes usar operaciones familiares como consultas SQL o transformaciones tipo DataFrame para procesar estos flujos, igual que con datos estáticos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caracteristicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API simple y familiar:\n",
    "\n",
    "- Usa las mismas API que Spark SQL y DataFrames, lo que simplifica mucho la curva de aprendizaje.\n",
    "- Ejemplo: Puedes filtrar, agrupar o transformar datos en tiempo real como si fueran tablas.\n",
    "### Tolerancia a fallos:\n",
    "\n",
    "- Garantiza que los datos se procesen exactamente una vez, incluso si ocurre un fallo en el sistema.\n",
    "### Procesamiento incremental:\n",
    "\n",
    "- Solo procesa los datos nuevos que han llegado desde la última operación, optimizando el rendimiento.\n",
    "### Soporte para fuentes y sinks populares:\n",
    "\n",
    "- Fuentes (entrada): Kafka, sockets, directorios de archivos, Kinesis, etc.\n",
    "- Sinks (salida): Consola, archivos, Kafka, bases de datos, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcionamiento:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Fuentes de datos en streaming:**  \n",
    "Los datos fluyen desde una fuente (como Kafka o un socket) hacia Spark en forma de eventos o registros.\n",
    "\n",
    "2. **Procesamiento continuo:**  \n",
    "Los datos se procesan en micro-batches (por defecto) o mediante un procesamiento completamente continuo (modo experimental).\n",
    "\n",
    "3. **Escritura en un destino:**  \n",
    "Después de procesar los datos, los resultados se envían a un destino (como una base de datos o un sistema de archivos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estructura general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de libreria\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import * \n",
    "\n",
    "# crear contexto\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MiAplicacionStreaming\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Definir la fuente de datos\n",
    "df = spark.readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# Mostrar los datos leídos en esta etapa podemos guardar los datos ( en general desidimos que hacer con ellos )\n",
    "df.select(\"value\").writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .start() \\\n",
    "    .awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
