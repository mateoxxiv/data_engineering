{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "diseñado para procesar datos estructurados y realizar consultas SQL en grandes volúmenes de datos. Es una parte fundamental del ecosistema de Spark que permite a los desarrolladores y científicos de datos trabajar con datos de manera eficiente y flexible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuentes de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Archivos CSV\n",
    "- Archivos JSON\n",
    "- Archivos Parquet\n",
    "- Archivos Avro\n",
    "- Bases de datos SQL (como PostgreSQL, MySQL, SQL Server)\n",
    "- Bases de datos NoSQL (como MongoDB, Cassandra)\n",
    "- Apache Hive\n",
    "- Apache HBase\n",
    "- Archivos de texto plano\n",
    "- Fuentes de datos en la nube (como Amazon S3, Google Cloud Storage, Azure Blob Storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### crear un contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crear una Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EjemploPySparkCSV\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conexión a un archivo csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordenes_ruta = '../resources/ordenes.csv'\n",
    "ordenes = spark.read.csv(ordenes_ruta, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear una tabla temporal para hacer consultas sobre ella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le qsuganmos nombres a las columnas\n",
    "ordenes = ordenes.withColumnRenamed('', 'row') \\\n",
    "    .withColumnRenamed('0', 'id') \\\n",
    "    .withColumnRenamed('1', 'cliente_id') \\\n",
    "    .withColumnRenamed('2', 'valor') \\\n",
    "    .withColumnRenamed('3', 'fecha_compra') \\\n",
    "    .withColumnRenamed('4', 'status')\n",
    "\n",
    "ordenes.createOrReplaceTempView(\"tabla_temporal\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizar consutas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado = spark.sql(\"SELECT * FROM tabla_temporal limit 3 \")\n",
    "resultado.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otras funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregar una nueva columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "ordenes = ordenes.withColumn('unos', lit(1))\n",
    "ordenes.createOrReplaceTempView(\"tabla_temporal\")\n",
    "resultado = spark.sql(\"SELECT * FROM tabla_temporal where status <> 'Cancelada' limit 3 \")\n",
    "resultado.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleccionar columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = ordenes.select('valor', 'cliente_id')\n",
    "column.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrar filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pedientes = ordenes.filter(ordenes['status'] == 'Pendiente')\n",
    "pedientes.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordenes.groupBy('status').sum('valor').show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podemos hacer joins entre DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = df.join(otro_df, df[\"columna_comun\"] == otro_df[\"columna_comun\"], \"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de agregación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicar una ó mas funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** funcitons literalmente me trae toas las funciones con las que puedo operar mis datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "ordenes.groupBy('status').agg(\n",
    "    F.sum('valor'), \n",
    "    F.avg('valor'), \n",
    "    F.min('valor'), \n",
    "    F.max('valor')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicar una fuinción de agregación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordenes.groupBy('status').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de fecha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordenes = ordenes.withColumn('fecha_compra_format', F.to_timestamp(ordenes['fecha_compra']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Componetes de la fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordenes = ordenes.withColumn('year', F.year('fecha_compra_format'))\n",
    "ordenes = ordenes.withColumn('month', F.month('fecha_compra_format'))\n",
    "ordenes = ordenes.withColumn('day', F.day('fecha_compra_format'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ordenes.select('day')\n",
    "response.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operar con texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ordenes.withColumn('upper_name', F.lower('status'))\n",
    "response.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones ventana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window = Window.partitionBy('year').orderBy('id')\n",
    "\n",
    "response = ordenes.withColumn('numero_columna', F.lag('valor',1).over(window))\n",
    "response.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elimininar una columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = response.drop('year', 'month', 'day', 'numero_columna')\n",
    "response.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ver el esquema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de WHERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar filter ó where de forma indistintiva en cualquier situación en la que queramos filtrar información  de neustra columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.where(\"cliente_id = 2 and fecha_compra_format >= '2024-01-01'\").show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
